{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mloda + scikit-learn Integration: Basic Example\n",
    "\n",
    "This notebook demonstrates how mloda enhances scikit-learn workflows by providing reusable, manageable feature transformations.\n",
    "\n",
    "## Quick Comparison: Traditional sklearn vs mloda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data with missing values:\n",
      "    age     weight state gender\n",
      "0  56.0  90.585667    NY      F\n",
      "1  69.0  59.833209    NY      M\n",
      "2  46.0  87.302978    FL      F\n",
      "3  32.0  64.374841    FL      M\n",
      "4  60.0  59.587811    FL      M\n",
      "\n",
      "Data shape: (1000, 4)\n",
      "Missing values: age=50, weight=50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create realistic sample data with missing values\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data_dict = {\n",
    "    \"age\": np.random.randint(18, 80, n_samples),\n",
    "    \"weight\": np.random.normal(70, 15, n_samples),\n",
    "    \"state\": np.random.choice([\"CA\", \"NY\", \"TX\", \"FL\"], n_samples),\n",
    "    \"gender\": np.random.choice([\"M\", \"F\"], n_samples),\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "# Introduce missing values\n",
    "missing_indices = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\n",
    "data.loc[missing_indices[:50], \"age\"] = np.nan\n",
    "data.loc[missing_indices[50:], \"weight\"] = np.nan\n",
    "\n",
    "print(\"Sample data with missing values:\")\n",
    "print(data.head())\n",
    "print(f\"\\nData shape: {data.shape}\")\n",
    "print(f\"Missing values: age={data['age'].isna().sum()}, weight={data['weight'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional scikit-learn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformed dataset with split fit/transform:\n",
      "   state_CA  state_FL  state_NY  state_TX  gender_F  gender_M       age  \\\n",
      "0       0.0       0.0       1.0       0.0       1.0       0.0  0.351358   \n",
      "1       0.0       0.0       1.0       0.0       0.0       1.0  1.086080   \n",
      "\n",
      "     weight  \n",
      "0  1.287800  \n",
      "1 -0.705904  \n",
      "Traditional pipeline result shape: (1000, 8)\n",
      "Result: 8 columns total\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# numeric_preprocessor = Pipeline(\n",
    "numeric_preprocessor = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputation_mean\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "categorical_preprocessor = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"imputation_constant\",\n",
    "            SimpleImputer(fill_value=\"missing\", strategy=\"constant\"),\n",
    "        ),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"categorical\", categorical_preprocessor, [\"state\", \"gender\"]),\n",
    "        (\"numerical\", numeric_preprocessor, [\"age\", \"weight\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "preprocessor.fit(data)  # Learn imputations, encoder categories, and scaler parameters\n",
    "X_transformed = preprocessor.transform(data)  # Apply the transformations\n",
    "\n",
    "onehot_feature_names = (\n",
    "    preprocessor.named_transformers_[\"categorical\"].named_steps[\"onehot\"].get_feature_names_out([\"state\", \"gender\"])\n",
    ")\n",
    "numeric_feature_names = [\"age\", \"weight\"]\n",
    "all_feature_names = np.concatenate([onehot_feature_names, numeric_feature_names])\n",
    "\n",
    "df_transformed = pd.DataFrame(\n",
    "    X_transformed.toarray() if hasattr(X_transformed, \"toarray\") else X_transformed,  # type: ignore\n",
    "    columns=all_feature_names,\n",
    ")\n",
    "\n",
    "print(\"✅ Transformed dataset with split fit/transform:\")\n",
    "print(df_transformed.head(2))\n",
    "\n",
    "print(f\"Traditional pipeline result shape: {X_transformed.shape}\")\n",
    "print(f\"Result: {len(df_transformed.columns)} columns total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mloda Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/envs/python310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformed dataset with split fit/transform:\n",
      "   onehot_encoded__state~3  onehot_encoded__state~0  onehot_encoded__gender~1  \\\n",
      "0                      0.0                      0.0                       0.0   \n",
      "1                      0.0                      0.0                       1.0   \n",
      "\n",
      "   onehot_encoded__state~1  onehot_encoded__state~2  onehot_encoded__gender~0  \n",
      "0                      0.0                      1.0                       1.0  \n",
      "1                      0.0                      1.0                       0.0  \n",
      "   standard_scaled__mean_imputed__age  standard_scaled__mean_imputed__weight\n",
      "0                            0.339295                               1.243621\n",
      "1                            1.057320                              -0.677472\n",
      "Result: ['onehot_encoded__state~3', 'onehot_encoded__state~0', 'onehot_encoded__gender~1', 'onehot_encoded__state~1', 'onehot_encoded__state~2', 'onehot_encoded__gender~0'] \n",
      " ['standard_scaled__mean_imputed__age', 'standard_scaled__mean_imputed__weight'] columns total\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Any\n",
    "from mloda_core.abstract_plugins.abstract_feature_group import AbstractFeatureGroup\n",
    "from mloda_core.abstract_plugins.components.input_data.base_input_data import BaseInputData\n",
    "from mloda_core.abstract_plugins.components.input_data.creator.data_creator import DataCreator\n",
    "from mloda_core.abstract_plugins.components.feature_set import FeatureSet\n",
    "from mloda_core.abstract_plugins.plugin_loader.plugin_loader import PluginLoader\n",
    "from mloda_core.api.request import mlodaAPI\n",
    "from mloda_plugins.compute_framework.base_implementations.pandas.dataframe import PandasDataframe\n",
    "\n",
    "# In mloda, we have the concept of feature groups.\n",
    "# A feature group is an abstraction between a data framework and processes of a data transformation.\n",
    "# In this example, the data framework is clearly pandas.\n",
    "# The processes are typically meta information like names, but lifecyle definition, or dependencies or relations to other data.\n",
    "\n",
    "# On the basis on the given data_dict earlier defined and its names, we use a DataCreator to inject the data_dict into the feature group abstraction.\n",
    "# Very simply spoken: we load the data.\n",
    "\n",
    "\n",
    "class SklearnDataCreator(AbstractFeatureGroup):\n",
    "    # This function is core to mloda. In this spot, the data framework with the actual data representation meets the defined and resolved processes.\n",
    "    # With this, we have access to the before and after state of a feature.\n",
    "\n",
    "    @classmethod\n",
    "    def calculate_feature(cls, data: Any, features: FeatureSet) -> Any:\n",
    "        # If this feature would not load data, we would use the data given from the parameter \"data\".\n",
    "        data = pd.DataFrame(data_dict)\n",
    "        return data\n",
    "\n",
    "    # One way to get this data is via defined input_data. But there are many more and we should not go to deep into this topic for now.\n",
    "    @classmethod\n",
    "    def input_data(cls) -> Optional[BaseInputData]:\n",
    "        return DataCreator({\"age\", \"weight\", \"state\", \"gender\"})\n",
    "\n",
    "\n",
    "# As next, we will use one method of defining what features we want as result from the mloda framework.\n",
    "features = [\n",
    "    \"standard_scaled__mean_imputed__age\",  # Scale imputed age\n",
    "    \"standard_scaled__mean_imputed__weight\",  # Scale imputed weight\n",
    "    \"onehot_encoded__state\",  # One-hot encode state\n",
    "    \"onehot_encoded__gender\",  # One-hot encode gender\n",
    "]\n",
    "\n",
    "# We now use a trick to register all known feature groups. mloda will only use those which are loaded into the namespace.\n",
    "PluginLoader().all()\n",
    "\n",
    "# And then we execute mloda, which will resolve its dependencies of its feature groups and data frame technologies automatically.\n",
    "result = mlodaAPI.run_all(features, compute_frameworks={PandasDataframe})\n",
    "_result, _result2 = result[0], result[1]\n",
    "print(\"✅ Transformed dataset with split fit/transform:\")\n",
    "print(_result.head(2))\n",
    "print(_result2.head(2))\n",
    "\n",
    "print(f\"Result: {list(_result.columns)} \\n {list(_result2.columns)} columns total\")\n",
    "\n",
    "# Remark 1: We have not yet added the functionality to map the value to a column string back. It is planned. https://github.com/TomKaltofen/mloda/issues/46\n",
    "\n",
    "# Remark 2: If you see the error \"ValueError: Multiple feature groups\", please restart the notebook. This happens if we load the class SklearnDataCreator twice into the notebook memory.\n",
    "#         I have yet to find a solution for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   onehot_encoded__state~0\n",
      "0                      0.0\n",
      "1                      0.0    robust_scaled__mean_imputed__weight\n",
      "0                             0.938858\n",
      "1                            -0.535025    max_aggr__standard_scaled__mean_imputed__age\n",
      "0                                      1.609647\n",
      "1                                      1.609647\n"
     ]
    }
   ],
   "source": [
    "# The beauty and strength of mloda is that we can combine feature groups in a very creative way.\n",
    "chained_features = [\n",
    "    \"max_aggr__standard_scaled__mean_imputed__age\",  # Do feature pipelines\n",
    "    \"robust_scaled__mean_imputed__weight\",  # Different scaler for weight\n",
    "    \"onehot_encoded__state~0\",  # Access specific one-hot column\n",
    "]\n",
    "\n",
    "result = mlodaAPI.run_all(chained_features, compute_frameworks={PandasDataframe})\n",
    "print(\n",
    "    result[0].head(2),\n",
    "    result[1].head(2),\n",
    "    result[2].head(2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I AM NOW USED.\n",
      "   onehot_encoded__state~0\n",
      "0                      1.0\n",
      "1                      1.0    robust_scaled__mean_imputed__weight\n",
      "0                             0.677809\n",
      "1                            -0.293511    max_aggr__standard_scaled__mean_imputed__age\n",
      "0                                      1.631997\n",
      "1                                      1.631997\n"
     ]
    }
   ],
   "source": [
    "from mloda_core.abstract_plugins.components.plugin_option.plugin_collector import PlugInCollector\n",
    "\n",
    "\n",
    "class SecondSklearnDataCreator(AbstractFeatureGroup):\n",
    "    @classmethod\n",
    "    def input_data(cls) -> Optional[BaseInputData]:\n",
    "        return DataCreator({\"age\", \"weight\", \"state\", \"gender\"})\n",
    "\n",
    "    @classmethod\n",
    "    def calculate_feature(cls, data: Any, features: FeatureSet) -> Any:\n",
    "        print(f\"I, {cls.get_class_name()} AM NOW USED.\")\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"age\": np.random.randint(25, 65, 500),\n",
    "                \"weight\": np.random.normal(80, 20, 500),  # Different distribution\n",
    "                \"state\": np.random.choice([\"WA\", \"OR\"], 500),  # Different states!\n",
    "                \"gender\": np.random.choice([\"M\", \"F\", \"Other\"], 500),  # New category!\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "chained_features = [\n",
    "    \"max_aggr__standard_scaled__mean_imputed__age\",  # Step 2: Scale imputed\n",
    "    \"robust_scaled__mean_imputed__weight\",  # Different scaler for weight\n",
    "    \"onehot_encoded__state~0\",  # Access specific one-hot column\n",
    "]\n",
    "\n",
    "# We deactivated now the other feature group, so that we use SecondSklearnDataCreator.\n",
    "result = mlodaAPI.run_all(\n",
    "    chained_features,\n",
    "    compute_frameworks={PandasDataframe},\n",
    "    plugin_collector=PlugInCollector.disabled_feature_groups(SklearnDataCreator),\n",
    ")\n",
    "print(\n",
    "    result[0].head(2),\n",
    "    result[1].head(2),\n",
    "    result[2].head(2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences\n",
    "\n",
    "### Traditional sklearn:\n",
    "- Complex nested pipelines with manual step coordination\n",
    "- Imputation → scaling → encoding workflow requires careful management\n",
    "- Pipeline tied to specific dataset\n",
    "\n",
    "### mloda:\n",
    "- Each transformation becomes a **feature**\n",
    "- Feature chaining: `standard_scaled__mean_imputed__age`\n",
    "- Automatic dependency resolution and artifact management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Reusability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional: Worked (but uses old fitted parameters)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DataAccessCollection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraditional: Failed - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)[:\u001b[38;5;241m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# mloda: Same feature definitions work automatically\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m new_data_access \u001b[38;5;241m=\u001b[39m \u001b[43mDataAccessCollection\u001b[49m()\n\u001b[1;32m     23\u001b[0m new_data_access\u001b[38;5;241m.\u001b[39madd_data_source(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_data)\n\u001b[1;32m     25\u001b[0m same_features \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_imputed__weight\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard_scaled__mean_imputed__weight\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monehot_encoded__state\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monehot_encoded__gender\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataAccessCollection' is not defined"
     ]
    }
   ],
   "source": [
    "# New dataset with different distribution and missing values\n",
    "new_data = pd.DataFrame(\n",
    "    {\n",
    "        \"age\": np.random.randint(25, 65, 500),\n",
    "        \"weight\": np.random.normal(80, 20, 500),  # Different distribution\n",
    "        \"state\": np.random.choice([\"WA\", \"OR\"], 500),  # Different states!\n",
    "        \"gender\": np.random.choice([\"M\", \"F\", \"Other\"], 500),  # New category!\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add different missing value pattern\n",
    "new_data.loc[:20, \"weight\"] = np.nan\n",
    "\n",
    "# Traditional: Need to refit for new data distribution and categories\n",
    "try:\n",
    "    preprocessor.transform(new_data[:5])\n",
    "    print(\"Traditional: Worked (but uses old fitted parameters)\")\n",
    "except Exception as e:\n",
    "    print(f\"Traditional: Failed - {str(e)[:50]}...\")\n",
    "\n",
    "# mloda: Same feature definitions work automatically\n",
    "new_data_access = DataAccessCollection()\n",
    "new_data_access.add_data_source(\"new_data\", new_data)\n",
    "\n",
    "same_features = [\n",
    "    \"mean_imputed__weight\",\n",
    "    \"standard_scaled__mean_imputed__weight\",\n",
    "    \"onehot_encoded__state\",\n",
    "    \"onehot_encoded__gender\",\n",
    "]\n",
    "\n",
    "new_result = mlodaAPI.run_all(\n",
    "    same_features, data_access_collection=new_data_access, compute_frameworks={PandasDataframe}\n",
    ")\n",
    "\n",
    "print(f\"mloda: Handles new data automatically! Shape: {new_result[0].shape}\")\n",
    "print(f\"New categories handled: {[col for col in new_result[0].columns if 'onehot_encoded__gender' in col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Chaining: The Real Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature chaining: The real power of mloda\n",
    "chained_features = [\n",
    "    \"age\",\n",
    "    \"weight\",\n",
    "    \"mean_imputed__age\",  # Step 1: Impute\n",
    "    \"standard_scaled__mean_imputed__age\",  # Step 2: Scale imputed\n",
    "    \"robust_scaled__mean_imputed__weight\",  # Different scaler for weight\n",
    "    \"onehot_encoded__state\",  # Encode categorical\n",
    "    \"onehot_encoded__state~0\",  # Access specific one-hot column\n",
    "]\n",
    "\n",
    "chained_result = mlodaAPI.run_all(\n",
    "    chained_features, data_access_collection=data_access, compute_frameworks={PandasDataframe}\n",
    ")\n",
    "\n",
    "print(\"Feature chaining example:\")\n",
    "chained_data = chained_result[0]\n",
    "print(f\"Original age: {chained_data['age'].iloc[0]:.2f}\")\n",
    "print(f\"Imputed age: {chained_data['mean_imputed__age'].iloc[0]:.2f}\")\n",
    "print(f\"Scaled age: {chained_data['standard_scaled__mean_imputed__age'].iloc[0]:.2f}\")\n",
    "print(f\"Available columns: {len(chained_data.columns)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "mloda transforms sklearn preprocessing from:\n",
    "- **Complex nested pipelines** → **Simple feature chains**\n",
    "- **Manual step coordination** → **Automatic dependency resolution**\n",
    "- **Imperative code** → **Declarative feature names**\n",
    "- **Dataset-specific** → **Reusable across projects**\n",
    "\n",
    "Same sklearn transformations, much better workflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
